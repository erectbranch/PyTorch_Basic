{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch로 neural network를 훈련하기: MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.1 data set loading\n",
    "\n",
    "`DataLoader` 모듈을 사용해서 간단히 load할 수 있다.\n",
    "\n",
    "> batch size는 보편적으로 사용하는 32로 지정했다. tradeoff가 있기 때문에 적절한 값을 사용해야 한다.\n",
    "\n",
    "> 또한 **normalize**(정규화)를 적용하여 정규분포를 따르도록 수정했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,))])), # train_X.mean()/256. and train_X.std()/256.\n",
    "    batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,)) \n",
    "                   ])),\n",
    "    batch_size=500, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.2 model architecture 정의\n",
    "\n",
    "![model architecture](images/CNN_architecture.png)\n",
    "\n",
    "- input image: (28X28X1) gray scale\n",
    "\n",
    "- 첫 번째 convolutional layer: (26X26X16)\n",
    "\n",
    "  - input channel 1, output channel 16, kernel size 3, stride 1\n",
    "\n",
    "  - activation function: ReLU\n",
    "\n",
    "- 두 번째 convolutional layer: (24X24X32)\n",
    "\n",
    "  - input channel 16, output channel 32, kernel size 3, stride 1\n",
    "\n",
    "  - activation function: ReLU\n",
    "\n",
    "> 사실 kernel size가 5X5인 convolutional layer와 공간 커버리지 관점에서는 동일하다. 하지만 이렇게 여러 layer로 구성된 깊이가 더 깊은 network를 사용하면 더 복잡한 feature를 학습할 수 있다. 또한 parameter 수도 줄일 수 있다.\n",
    "\n",
    "다음은 model architecture를 구현한 code이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cn1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.dp1 = nn.Dropout2d(0.10)\n",
    "        self.dp2 = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(4608, 64)   # 12 X 12 X 32 = 4608\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.cn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dp1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp2(x)\n",
    "        x = self.fc2(x)\n",
    "        op = F.log_softmax(x, dim=1)\n",
    "        return op"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.3 training routine\n",
    "\n",
    "실제 traning routine, 즉 실제 backpropagation 단계를 정의한다. `torch.optim` 모듈을 이용해 간결한 code로 구성한다.\n",
    "\n",
    "다음은 train data set에 사용할 routine이다. 이 과정을 한 번 수행하는 것을 1 **epoch**(세대)라고 한다.\n",
    "\n",
    "> 다시 말해 여기서 1 epoch는 data set 전체를 읽는 시간을 뜻한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_dataloader, optim, epoch):\n",
    "    model.train()\n",
    "    for b_i, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optim.zero_grad()\n",
    "        pred_prob = model(X)            # 예측값\n",
    "        loss = F.nll_loss(pred_prob, y) # nll: negative likelihood loss\n",
    "        loss.backward()                 # gradients 계산\n",
    "        optim.step()                    # weight update\n",
    "        if b_i % 10 == 0:               # batch 10개마다 training loss를 출력한다.\n",
    "            print('epoch: {} [{}/{} ({:.0f}%)]\\t training loss: {:.6f}'.format(\n",
    "                epoch, b_i * len(X), len(train_dataloader.dataset),\n",
    "                100. * b_i / len(train_dataloader), loss.item()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.4 test routine\n",
    "\n",
    "다음은 test data set에서 사용할 routine이다.\n",
    "\n",
    "> train routine과의 차이는 계산한 loss를 사용해서 update를 수행하지 않는다는 점이다. 대신 loss는 전체 test error 총합을 위해 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    success = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred_prob = model(X)\n",
    "            loss += F.nll_loss(pred_prob, y, reduction='sum').item()  # loss summed across the batch\n",
    "            pred = pred_prob.argmax(dim=1, keepdim=True)  # us argmax to get the most likely prediction\n",
    "            success += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    loss /= len(test_dataloader.dataset)\n",
    "\n",
    "    print('\\nTest dataset: Overall Loss: {:.4f}, Overall Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss, success, len(test_dataloader.dataset),\n",
    "        100. * success / len(test_dataloader.dataset)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.5 optimizer와 device 정의\n",
    "\n",
    "optimizer로 AdaDelta를 사용하고 learning rate를 0.5로 설정했다. 장치는 CPU를 사용한다.\n",
    "\n",
    "> data가 sparsity를 가질 경우 AdaDelta가 유용하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = ConvNet()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.6 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.310609\n",
      "epoch: 1 [320/60000 (1%)]\t training loss: 1.924132\n",
      "epoch: 1 [640/60000 (1%)]\t training loss: 1.313337\n",
      "epoch: 1 [960/60000 (2%)]\t training loss: 0.796470\n",
      "epoch: 1 [1280/60000 (2%)]\t training loss: 0.819801\n",
      "epoch: 1 [1600/60000 (3%)]\t training loss: 0.678459\n",
      "epoch: 1 [1920/60000 (3%)]\t training loss: 0.477065\n",
      "epoch: 1 [2240/60000 (4%)]\t training loss: 0.527093\n",
      "epoch: 1 [2560/60000 (4%)]\t training loss: 0.469393\n",
      "epoch: 1 [2880/60000 (5%)]\t training loss: 0.239183\n",
      "epoch: 1 [3200/60000 (5%)]\t training loss: 0.523874\n",
      "epoch: 1 [3520/60000 (6%)]\t training loss: 0.269437\n",
      "epoch: 1 [3840/60000 (6%)]\t training loss: 0.464022\n",
      "epoch: 1 [4160/60000 (7%)]\t training loss: 0.413103\n",
      "epoch: 1 [4480/60000 (7%)]\t training loss: 0.326608\n",
      "epoch: 1 [4800/60000 (8%)]\t training loss: 0.493001\n",
      "epoch: 1 [5120/60000 (9%)]\t training loss: 0.154175\n",
      "epoch: 1 [5440/60000 (9%)]\t training loss: 0.356458\n",
      "epoch: 1 [5760/60000 (10%)]\t training loss: 0.092037\n",
      "epoch: 1 [6080/60000 (10%)]\t training loss: 0.183583\n",
      "epoch: 1 [6400/60000 (11%)]\t training loss: 0.291309\n",
      "epoch: 1 [6720/60000 (11%)]\t training loss: 0.085847\n",
      "epoch: 1 [7040/60000 (12%)]\t training loss: 0.340037\n",
      "epoch: 1 [7360/60000 (12%)]\t training loss: 0.042312\n",
      "epoch: 1 [7680/60000 (13%)]\t training loss: 0.383257\n",
      "epoch: 1 [8000/60000 (13%)]\t training loss: 0.234551\n",
      "epoch: 1 [8320/60000 (14%)]\t training loss: 0.323864\n",
      "epoch: 1 [8640/60000 (14%)]\t training loss: 0.215861\n",
      "epoch: 1 [8960/60000 (15%)]\t training loss: 0.059165\n",
      "epoch: 1 [9280/60000 (15%)]\t training loss: 0.373748\n",
      "epoch: 1 [9600/60000 (16%)]\t training loss: 0.228409\n",
      "epoch: 1 [9920/60000 (17%)]\t training loss: 0.217323\n",
      "epoch: 1 [10240/60000 (17%)]\t training loss: 0.201431\n",
      "epoch: 1 [10560/60000 (18%)]\t training loss: 0.392330\n",
      "epoch: 1 [10880/60000 (18%)]\t training loss: 0.076165\n",
      "epoch: 1 [11200/60000 (19%)]\t training loss: 0.058499\n",
      "epoch: 1 [11520/60000 (19%)]\t training loss: 0.231726\n",
      "epoch: 1 [11840/60000 (20%)]\t training loss: 0.047101\n",
      "epoch: 1 [12160/60000 (20%)]\t training loss: 0.406783\n",
      "epoch: 1 [12480/60000 (21%)]\t training loss: 0.024059\n",
      "epoch: 1 [12800/60000 (21%)]\t training loss: 0.098933\n",
      "epoch: 1 [13120/60000 (22%)]\t training loss: 0.092050\n",
      "epoch: 1 [13440/60000 (22%)]\t training loss: 0.160732\n",
      "epoch: 1 [13760/60000 (23%)]\t training loss: 0.071859\n",
      "epoch: 1 [14080/60000 (23%)]\t training loss: 0.049288\n",
      "epoch: 1 [14400/60000 (24%)]\t training loss: 0.078043\n",
      "epoch: 1 [14720/60000 (25%)]\t training loss: 0.160995\n",
      "epoch: 1 [15040/60000 (25%)]\t training loss: 0.336904\n",
      "epoch: 1 [15360/60000 (26%)]\t training loss: 0.076593\n",
      "epoch: 1 [15680/60000 (26%)]\t training loss: 0.093248\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 0.086215\n",
      "epoch: 1 [16320/60000 (27%)]\t training loss: 0.161077\n",
      "epoch: 1 [16640/60000 (28%)]\t training loss: 0.047388\n",
      "epoch: 1 [16960/60000 (28%)]\t training loss: 0.291342\n",
      "epoch: 1 [17280/60000 (29%)]\t training loss: 0.057895\n",
      "epoch: 1 [17600/60000 (29%)]\t training loss: 0.037908\n",
      "epoch: 1 [17920/60000 (30%)]\t training loss: 0.043249\n",
      "epoch: 1 [18240/60000 (30%)]\t training loss: 0.031522\n",
      "epoch: 1 [18560/60000 (31%)]\t training loss: 0.057310\n",
      "epoch: 1 [18880/60000 (31%)]\t training loss: 0.213368\n",
      "epoch: 1 [19200/60000 (32%)]\t training loss: 0.085842\n",
      "epoch: 1 [19520/60000 (33%)]\t training loss: 0.073042\n",
      "epoch: 1 [19840/60000 (33%)]\t training loss: 0.069544\n",
      "epoch: 1 [20160/60000 (34%)]\t training loss: 0.111760\n",
      "epoch: 1 [20480/60000 (34%)]\t training loss: 0.070013\n",
      "epoch: 1 [20800/60000 (35%)]\t training loss: 0.205984\n",
      "epoch: 1 [21120/60000 (35%)]\t training loss: 0.267319\n",
      "epoch: 1 [21440/60000 (36%)]\t training loss: 0.258662\n",
      "epoch: 1 [21760/60000 (36%)]\t training loss: 0.102205\n",
      "epoch: 1 [22080/60000 (37%)]\t training loss: 0.081193\n",
      "epoch: 1 [22400/60000 (37%)]\t training loss: 0.140008\n",
      "epoch: 1 [22720/60000 (38%)]\t training loss: 0.275206\n",
      "epoch: 1 [23040/60000 (38%)]\t training loss: 0.061010\n",
      "epoch: 1 [23360/60000 (39%)]\t training loss: 0.120260\n",
      "epoch: 1 [23680/60000 (39%)]\t training loss: 0.268303\n",
      "epoch: 1 [24000/60000 (40%)]\t training loss: 0.181778\n",
      "epoch: 1 [24320/60000 (41%)]\t training loss: 0.059005\n",
      "epoch: 1 [24640/60000 (41%)]\t training loss: 0.141283\n",
      "epoch: 1 [24960/60000 (42%)]\t training loss: 0.026737\n",
      "epoch: 1 [25280/60000 (42%)]\t training loss: 0.020366\n",
      "epoch: 1 [25600/60000 (43%)]\t training loss: 0.550593\n",
      "epoch: 1 [25920/60000 (43%)]\t training loss: 0.476069\n",
      "epoch: 1 [26240/60000 (44%)]\t training loss: 0.106938\n",
      "epoch: 1 [26560/60000 (44%)]\t training loss: 0.045155\n",
      "epoch: 1 [26880/60000 (45%)]\t training loss: 0.137116\n",
      "epoch: 1 [27200/60000 (45%)]\t training loss: 0.012286\n",
      "epoch: 1 [27520/60000 (46%)]\t training loss: 0.036071\n",
      "epoch: 1 [27840/60000 (46%)]\t training loss: 0.103347\n",
      "epoch: 1 [28160/60000 (47%)]\t training loss: 0.041360\n",
      "epoch: 1 [28480/60000 (47%)]\t training loss: 0.397994\n",
      "epoch: 1 [28800/60000 (48%)]\t training loss: 0.145114\n",
      "epoch: 1 [29120/60000 (49%)]\t training loss: 0.214860\n",
      "epoch: 1 [29440/60000 (49%)]\t training loss: 0.011588\n",
      "epoch: 1 [29760/60000 (50%)]\t training loss: 0.044677\n",
      "epoch: 1 [30080/60000 (50%)]\t training loss: 0.539932\n",
      "epoch: 1 [30400/60000 (51%)]\t training loss: 0.131117\n",
      "epoch: 1 [30720/60000 (51%)]\t training loss: 0.226433\n",
      "epoch: 1 [31040/60000 (52%)]\t training loss: 0.076488\n",
      "epoch: 1 [31360/60000 (52%)]\t training loss: 0.066237\n",
      "epoch: 1 [31680/60000 (53%)]\t training loss: 0.024391\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 0.010573\n",
      "epoch: 1 [32320/60000 (54%)]\t training loss: 0.145924\n",
      "epoch: 1 [32640/60000 (54%)]\t training loss: 0.084602\n",
      "epoch: 1 [32960/60000 (55%)]\t training loss: 0.061691\n",
      "epoch: 1 [33280/60000 (55%)]\t training loss: 0.252904\n",
      "epoch: 1 [33600/60000 (56%)]\t training loss: 0.174225\n",
      "epoch: 1 [33920/60000 (57%)]\t training loss: 0.018843\n",
      "epoch: 1 [34240/60000 (57%)]\t training loss: 0.022718\n",
      "epoch: 1 [34560/60000 (58%)]\t training loss: 0.004074\n",
      "epoch: 1 [34880/60000 (58%)]\t training loss: 0.127562\n",
      "epoch: 1 [35200/60000 (59%)]\t training loss: 0.247389\n",
      "epoch: 1 [35520/60000 (59%)]\t training loss: 0.147870\n",
      "epoch: 1 [35840/60000 (60%)]\t training loss: 0.018604\n",
      "epoch: 1 [36160/60000 (60%)]\t training loss: 0.158040\n",
      "epoch: 1 [36480/60000 (61%)]\t training loss: 0.104241\n",
      "epoch: 1 [36800/60000 (61%)]\t training loss: 0.122958\n",
      "epoch: 1 [37120/60000 (62%)]\t training loss: 0.034434\n",
      "epoch: 1 [37440/60000 (62%)]\t training loss: 0.225140\n",
      "epoch: 1 [37760/60000 (63%)]\t training loss: 0.035332\n",
      "epoch: 1 [38080/60000 (63%)]\t training loss: 0.139743\n",
      "epoch: 1 [38400/60000 (64%)]\t training loss: 0.056595\n",
      "epoch: 1 [38720/60000 (65%)]\t training loss: 0.464158\n",
      "epoch: 1 [39040/60000 (65%)]\t training loss: 0.046275\n",
      "epoch: 1 [39360/60000 (66%)]\t training loss: 0.137037\n",
      "epoch: 1 [39680/60000 (66%)]\t training loss: 0.023135\n",
      "epoch: 1 [40000/60000 (67%)]\t training loss: 0.132450\n",
      "epoch: 1 [40320/60000 (67%)]\t training loss: 0.070551\n",
      "epoch: 1 [40640/60000 (68%)]\t training loss: 0.202422\n",
      "epoch: 1 [40960/60000 (68%)]\t training loss: 0.165868\n",
      "epoch: 1 [41280/60000 (69%)]\t training loss: 0.201902\n",
      "epoch: 1 [41600/60000 (69%)]\t training loss: 0.178212\n",
      "epoch: 1 [41920/60000 (70%)]\t training loss: 0.039672\n",
      "epoch: 1 [42240/60000 (70%)]\t training loss: 0.009508\n",
      "epoch: 1 [42560/60000 (71%)]\t training loss: 0.058255\n",
      "epoch: 1 [42880/60000 (71%)]\t training loss: 0.102172\n",
      "epoch: 1 [43200/60000 (72%)]\t training loss: 0.055403\n",
      "epoch: 1 [43520/60000 (73%)]\t training loss: 0.253508\n",
      "epoch: 1 [43840/60000 (73%)]\t training loss: 0.041966\n",
      "epoch: 1 [44160/60000 (74%)]\t training loss: 0.155540\n",
      "epoch: 1 [44480/60000 (74%)]\t training loss: 0.069957\n",
      "epoch: 1 [44800/60000 (75%)]\t training loss: 0.287161\n",
      "epoch: 1 [45120/60000 (75%)]\t training loss: 0.016246\n",
      "epoch: 1 [45440/60000 (76%)]\t training loss: 0.182881\n",
      "epoch: 1 [45760/60000 (76%)]\t training loss: 0.004939\n",
      "epoch: 1 [46080/60000 (77%)]\t training loss: 0.229007\n",
      "epoch: 1 [46400/60000 (77%)]\t training loss: 0.039858\n",
      "epoch: 1 [46720/60000 (78%)]\t training loss: 0.090354\n",
      "epoch: 1 [47040/60000 (78%)]\t training loss: 0.077319\n",
      "epoch: 1 [47360/60000 (79%)]\t training loss: 0.027445\n",
      "epoch: 1 [47680/60000 (79%)]\t training loss: 0.237214\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 0.235731\n",
      "epoch: 1 [48320/60000 (81%)]\t training loss: 0.162903\n",
      "epoch: 1 [48640/60000 (81%)]\t training loss: 0.017043\n",
      "epoch: 1 [48960/60000 (82%)]\t training loss: 0.095157\n",
      "epoch: 1 [49280/60000 (82%)]\t training loss: 0.147653\n",
      "epoch: 1 [49600/60000 (83%)]\t training loss: 0.242747\n",
      "epoch: 1 [49920/60000 (83%)]\t training loss: 0.122479\n",
      "epoch: 1 [50240/60000 (84%)]\t training loss: 0.068788\n",
      "epoch: 1 [50560/60000 (84%)]\t training loss: 0.003863\n",
      "epoch: 1 [50880/60000 (85%)]\t training loss: 0.004877\n",
      "epoch: 1 [51200/60000 (85%)]\t training loss: 0.210378\n",
      "epoch: 1 [51520/60000 (86%)]\t training loss: 0.023645\n",
      "epoch: 1 [51840/60000 (86%)]\t training loss: 0.010631\n",
      "epoch: 1 [52160/60000 (87%)]\t training loss: 0.009659\n",
      "epoch: 1 [52480/60000 (87%)]\t training loss: 0.009053\n",
      "epoch: 1 [52800/60000 (88%)]\t training loss: 0.031123\n",
      "epoch: 1 [53120/60000 (89%)]\t training loss: 0.030376\n",
      "epoch: 1 [53440/60000 (89%)]\t training loss: 0.030186\n",
      "epoch: 1 [53760/60000 (90%)]\t training loss: 0.037798\n",
      "epoch: 1 [54080/60000 (90%)]\t training loss: 0.006830\n",
      "epoch: 1 [54400/60000 (91%)]\t training loss: 0.093173\n",
      "epoch: 1 [54720/60000 (91%)]\t training loss: 0.008708\n",
      "epoch: 1 [55040/60000 (92%)]\t training loss: 0.001423\n",
      "epoch: 1 [55360/60000 (92%)]\t training loss: 0.048718\n",
      "epoch: 1 [55680/60000 (93%)]\t training loss: 0.050393\n",
      "epoch: 1 [56000/60000 (93%)]\t training loss: 0.041436\n",
      "epoch: 1 [56320/60000 (94%)]\t training loss: 0.167978\n",
      "epoch: 1 [56640/60000 (94%)]\t training loss: 0.077357\n",
      "epoch: 1 [56960/60000 (95%)]\t training loss: 0.044142\n",
      "epoch: 1 [57280/60000 (95%)]\t training loss: 0.091571\n",
      "epoch: 1 [57600/60000 (96%)]\t training loss: 0.003582\n",
      "epoch: 1 [57920/60000 (97%)]\t training loss: 0.094124\n",
      "epoch: 1 [58240/60000 (97%)]\t training loss: 0.036418\n",
      "epoch: 1 [58560/60000 (98%)]\t training loss: 0.006045\n",
      "epoch: 1 [58880/60000 (98%)]\t training loss: 0.080510\n",
      "epoch: 1 [59200/60000 (99%)]\t training loss: 0.005726\n",
      "epoch: 1 [59520/60000 (99%)]\t training loss: 0.024786\n",
      "epoch: 1 [59840/60000 (100%)]\t training loss: 0.035765\n",
      "\n",
      "Test dataset: Overall Loss: 0.0475, Overall Accuracy: 9841/10000 (98%)\n",
      "\n",
      "epoch: 2 [0/60000 (0%)]\t training loss: 0.083690\n",
      "epoch: 2 [320/60000 (1%)]\t training loss: 0.043185\n",
      "epoch: 2 [640/60000 (1%)]\t training loss: 0.139859\n",
      "epoch: 2 [960/60000 (2%)]\t training loss: 0.075948\n",
      "epoch: 2 [1280/60000 (2%)]\t training loss: 0.042603\n",
      "epoch: 2 [1600/60000 (3%)]\t training loss: 0.002129\n",
      "epoch: 2 [1920/60000 (3%)]\t training loss: 0.082193\n",
      "epoch: 2 [2240/60000 (4%)]\t training loss: 0.028921\n",
      "epoch: 2 [2560/60000 (4%)]\t training loss: 0.110363\n",
      "epoch: 2 [2880/60000 (5%)]\t training loss: 0.010131\n",
      "epoch: 2 [3200/60000 (5%)]\t training loss: 0.067806\n",
      "epoch: 2 [3520/60000 (6%)]\t training loss: 0.013564\n",
      "epoch: 2 [3840/60000 (6%)]\t training loss: 0.089675\n",
      "epoch: 2 [4160/60000 (7%)]\t training loss: 0.010817\n",
      "epoch: 2 [4480/60000 (7%)]\t training loss: 0.025331\n",
      "epoch: 2 [4800/60000 (8%)]\t training loss: 0.018246\n",
      "epoch: 2 [5120/60000 (9%)]\t training loss: 0.177433\n",
      "epoch: 2 [5440/60000 (9%)]\t training loss: 0.161318\n",
      "epoch: 2 [5760/60000 (10%)]\t training loss: 0.116250\n",
      "epoch: 2 [6080/60000 (10%)]\t training loss: 0.068412\n",
      "epoch: 2 [6400/60000 (11%)]\t training loss: 0.029646\n",
      "epoch: 2 [6720/60000 (11%)]\t training loss: 0.052700\n",
      "epoch: 2 [7040/60000 (12%)]\t training loss: 0.016197\n",
      "epoch: 2 [7360/60000 (12%)]\t training loss: 0.066184\n",
      "epoch: 2 [7680/60000 (13%)]\t training loss: 0.016993\n",
      "epoch: 2 [8000/60000 (13%)]\t training loss: 0.235030\n",
      "epoch: 2 [8320/60000 (14%)]\t training loss: 0.040008\n",
      "epoch: 2 [8640/60000 (14%)]\t training loss: 0.017027\n",
      "epoch: 2 [8960/60000 (15%)]\t training loss: 0.011108\n",
      "epoch: 2 [9280/60000 (15%)]\t training loss: 0.002585\n",
      "epoch: 2 [9600/60000 (16%)]\t training loss: 0.062505\n",
      "epoch: 2 [9920/60000 (17%)]\t training loss: 0.002975\n",
      "epoch: 2 [10240/60000 (17%)]\t training loss: 0.011998\n",
      "epoch: 2 [10560/60000 (18%)]\t training loss: 0.001980\n",
      "epoch: 2 [10880/60000 (18%)]\t training loss: 0.058645\n",
      "epoch: 2 [11200/60000 (19%)]\t training loss: 0.023311\n",
      "epoch: 2 [11520/60000 (19%)]\t training loss: 0.010396\n",
      "epoch: 2 [11840/60000 (20%)]\t training loss: 0.082448\n",
      "epoch: 2 [12160/60000 (20%)]\t training loss: 0.038417\n",
      "epoch: 2 [12480/60000 (21%)]\t training loss: 0.054934\n",
      "epoch: 2 [12800/60000 (21%)]\t training loss: 0.003958\n",
      "epoch: 2 [13120/60000 (22%)]\t training loss: 0.135544\n",
      "epoch: 2 [13440/60000 (22%)]\t training loss: 0.031498\n",
      "epoch: 2 [13760/60000 (23%)]\t training loss: 0.077387\n",
      "epoch: 2 [14080/60000 (23%)]\t training loss: 0.044983\n",
      "epoch: 2 [14400/60000 (24%)]\t training loss: 0.018181\n",
      "epoch: 2 [14720/60000 (25%)]\t training loss: 0.010663\n",
      "epoch: 2 [15040/60000 (25%)]\t training loss: 0.002013\n",
      "epoch: 2 [15360/60000 (26%)]\t training loss: 0.093999\n",
      "epoch: 2 [15680/60000 (26%)]\t training loss: 0.052174\n",
      "epoch: 2 [16000/60000 (27%)]\t training loss: 0.107595\n",
      "epoch: 2 [16320/60000 (27%)]\t training loss: 0.191513\n",
      "epoch: 2 [16640/60000 (28%)]\t training loss: 0.010545\n",
      "epoch: 2 [16960/60000 (28%)]\t training loss: 0.350629\n",
      "epoch: 2 [17280/60000 (29%)]\t training loss: 0.064236\n",
      "epoch: 2 [17600/60000 (29%)]\t training loss: 0.049724\n",
      "epoch: 2 [17920/60000 (30%)]\t training loss: 0.113266\n",
      "epoch: 2 [18240/60000 (30%)]\t training loss: 0.059003\n",
      "epoch: 2 [18560/60000 (31%)]\t training loss: 0.000723\n",
      "epoch: 2 [18880/60000 (31%)]\t training loss: 0.073392\n",
      "epoch: 2 [19200/60000 (32%)]\t training loss: 0.127392\n",
      "epoch: 2 [19520/60000 (33%)]\t training loss: 0.012418\n",
      "epoch: 2 [19840/60000 (33%)]\t training loss: 0.065102\n",
      "epoch: 2 [20160/60000 (34%)]\t training loss: 0.015453\n",
      "epoch: 2 [20480/60000 (34%)]\t training loss: 0.116335\n",
      "epoch: 2 [20800/60000 (35%)]\t training loss: 0.014909\n",
      "epoch: 2 [21120/60000 (35%)]\t training loss: 0.049169\n",
      "epoch: 2 [21440/60000 (36%)]\t training loss: 0.040008\n",
      "epoch: 2 [21760/60000 (36%)]\t training loss: 0.012706\n",
      "epoch: 2 [22080/60000 (37%)]\t training loss: 0.005037\n",
      "epoch: 2 [22400/60000 (37%)]\t training loss: 0.002095\n",
      "epoch: 2 [22720/60000 (38%)]\t training loss: 0.006831\n",
      "epoch: 2 [23040/60000 (38%)]\t training loss: 0.149872\n",
      "epoch: 2 [23360/60000 (39%)]\t training loss: 0.081530\n",
      "epoch: 2 [23680/60000 (39%)]\t training loss: 0.149554\n",
      "epoch: 2 [24000/60000 (40%)]\t training loss: 0.030978\n",
      "epoch: 2 [24320/60000 (41%)]\t training loss: 0.031637\n",
      "epoch: 2 [24640/60000 (41%)]\t training loss: 0.080633\n",
      "epoch: 2 [24960/60000 (42%)]\t training loss: 0.051087\n",
      "epoch: 2 [25280/60000 (42%)]\t training loss: 0.049625\n",
      "epoch: 2 [25600/60000 (43%)]\t training loss: 0.084059\n",
      "epoch: 2 [25920/60000 (43%)]\t training loss: 0.019303\n",
      "epoch: 2 [26240/60000 (44%)]\t training loss: 0.002812\n",
      "epoch: 2 [26560/60000 (44%)]\t training loss: 0.007962\n",
      "epoch: 2 [26880/60000 (45%)]\t training loss: 0.066617\n",
      "epoch: 2 [27200/60000 (45%)]\t training loss: 0.435735\n",
      "epoch: 2 [27520/60000 (46%)]\t training loss: 0.103941\n",
      "epoch: 2 [27840/60000 (46%)]\t training loss: 0.003215\n",
      "epoch: 2 [28160/60000 (47%)]\t training loss: 0.035698\n",
      "epoch: 2 [28480/60000 (47%)]\t training loss: 0.078579\n",
      "epoch: 2 [28800/60000 (48%)]\t training loss: 0.019156\n",
      "epoch: 2 [29120/60000 (49%)]\t training loss: 0.004184\n",
      "epoch: 2 [29440/60000 (49%)]\t training loss: 0.024789\n",
      "epoch: 2 [29760/60000 (50%)]\t training loss: 0.066620\n",
      "epoch: 2 [30080/60000 (50%)]\t training loss: 0.020286\n",
      "epoch: 2 [30400/60000 (51%)]\t training loss: 0.023729\n",
      "epoch: 2 [30720/60000 (51%)]\t training loss: 0.025549\n",
      "epoch: 2 [31040/60000 (52%)]\t training loss: 0.143278\n",
      "epoch: 2 [31360/60000 (52%)]\t training loss: 0.051958\n",
      "epoch: 2 [31680/60000 (53%)]\t training loss: 0.013630\n",
      "epoch: 2 [32000/60000 (53%)]\t training loss: 0.003489\n",
      "epoch: 2 [32320/60000 (54%)]\t training loss: 0.002814\n",
      "epoch: 2 [32640/60000 (54%)]\t training loss: 0.026823\n",
      "epoch: 2 [32960/60000 (55%)]\t training loss: 0.001378\n",
      "epoch: 2 [33280/60000 (55%)]\t training loss: 0.160555\n",
      "epoch: 2 [33600/60000 (56%)]\t training loss: 0.018991\n",
      "epoch: 2 [33920/60000 (57%)]\t training loss: 0.010739\n",
      "epoch: 2 [34240/60000 (57%)]\t training loss: 0.323032\n",
      "epoch: 2 [34560/60000 (58%)]\t training loss: 0.004089\n",
      "epoch: 2 [34880/60000 (58%)]\t training loss: 0.094320\n",
      "epoch: 2 [35200/60000 (59%)]\t training loss: 0.019374\n",
      "epoch: 2 [35520/60000 (59%)]\t training loss: 0.025495\n",
      "epoch: 2 [35840/60000 (60%)]\t training loss: 0.002630\n",
      "epoch: 2 [36160/60000 (60%)]\t training loss: 0.038855\n",
      "epoch: 2 [36480/60000 (61%)]\t training loss: 0.094803\n",
      "epoch: 2 [36800/60000 (61%)]\t training loss: 0.001759\n",
      "epoch: 2 [37120/60000 (62%)]\t training loss: 0.449823\n",
      "epoch: 2 [37440/60000 (62%)]\t training loss: 0.007574\n",
      "epoch: 2 [37760/60000 (63%)]\t training loss: 0.100398\n",
      "epoch: 2 [38080/60000 (63%)]\t training loss: 0.012099\n",
      "epoch: 2 [38400/60000 (64%)]\t training loss: 0.043760\n",
      "epoch: 2 [38720/60000 (65%)]\t training loss: 0.079969\n",
      "epoch: 2 [39040/60000 (65%)]\t training loss: 0.017762\n",
      "epoch: 2 [39360/60000 (66%)]\t training loss: 0.003121\n",
      "epoch: 2 [39680/60000 (66%)]\t training loss: 0.126887\n",
      "epoch: 2 [40000/60000 (67%)]\t training loss: 0.016442\n",
      "epoch: 2 [40320/60000 (67%)]\t training loss: 0.257851\n",
      "epoch: 2 [40640/60000 (68%)]\t training loss: 0.486474\n",
      "epoch: 2 [40960/60000 (68%)]\t training loss: 0.057167\n",
      "epoch: 2 [41280/60000 (69%)]\t training loss: 0.075897\n",
      "epoch: 2 [41600/60000 (69%)]\t training loss: 0.210954\n",
      "epoch: 2 [41920/60000 (70%)]\t training loss: 0.005607\n",
      "epoch: 2 [42240/60000 (70%)]\t training loss: 0.012966\n",
      "epoch: 2 [42560/60000 (71%)]\t training loss: 0.058290\n",
      "epoch: 2 [42880/60000 (71%)]\t training loss: 0.008833\n",
      "epoch: 2 [43200/60000 (72%)]\t training loss: 0.002852\n",
      "epoch: 2 [43520/60000 (73%)]\t training loss: 0.058913\n",
      "epoch: 2 [43840/60000 (73%)]\t training loss: 0.265866\n",
      "epoch: 2 [44160/60000 (74%)]\t training loss: 0.168544\n",
      "epoch: 2 [44480/60000 (74%)]\t training loss: 0.013209\n",
      "epoch: 2 [44800/60000 (75%)]\t training loss: 0.003294\n",
      "epoch: 2 [45120/60000 (75%)]\t training loss: 0.064749\n",
      "epoch: 2 [45440/60000 (76%)]\t training loss: 0.044105\n",
      "epoch: 2 [45760/60000 (76%)]\t training loss: 0.051989\n",
      "epoch: 2 [46080/60000 (77%)]\t training loss: 0.089703\n",
      "epoch: 2 [46400/60000 (77%)]\t training loss: 0.021513\n",
      "epoch: 2 [46720/60000 (78%)]\t training loss: 0.192908\n",
      "epoch: 2 [47040/60000 (78%)]\t training loss: 0.038975\n",
      "epoch: 2 [47360/60000 (79%)]\t training loss: 0.033707\n",
      "epoch: 2 [47680/60000 (79%)]\t training loss: 0.017389\n",
      "epoch: 2 [48000/60000 (80%)]\t training loss: 0.065331\n",
      "epoch: 2 [48320/60000 (81%)]\t training loss: 0.210205\n",
      "epoch: 2 [48640/60000 (81%)]\t training loss: 0.078638\n",
      "epoch: 2 [48960/60000 (82%)]\t training loss: 0.006712\n",
      "epoch: 2 [49280/60000 (82%)]\t training loss: 0.003017\n",
      "epoch: 2 [49600/60000 (83%)]\t training loss: 0.007529\n",
      "epoch: 2 [49920/60000 (83%)]\t training loss: 0.111704\n",
      "epoch: 2 [50240/60000 (84%)]\t training loss: 0.038250\n",
      "epoch: 2 [50560/60000 (84%)]\t training loss: 0.003947\n",
      "epoch: 2 [50880/60000 (85%)]\t training loss: 0.002682\n",
      "epoch: 2 [51200/60000 (85%)]\t training loss: 0.005808\n",
      "epoch: 2 [51520/60000 (86%)]\t training loss: 0.220550\n",
      "epoch: 2 [51840/60000 (86%)]\t training loss: 0.010954\n",
      "epoch: 2 [52160/60000 (87%)]\t training loss: 0.002295\n",
      "epoch: 2 [52480/60000 (87%)]\t training loss: 0.074274\n",
      "epoch: 2 [52800/60000 (88%)]\t training loss: 0.042485\n",
      "epoch: 2 [53120/60000 (89%)]\t training loss: 0.004128\n",
      "epoch: 2 [53440/60000 (89%)]\t training loss: 0.137196\n",
      "epoch: 2 [53760/60000 (90%)]\t training loss: 0.180501\n",
      "epoch: 2 [54080/60000 (90%)]\t training loss: 0.002374\n",
      "epoch: 2 [54400/60000 (91%)]\t training loss: 0.397189\n",
      "epoch: 2 [54720/60000 (91%)]\t training loss: 0.085437\n",
      "epoch: 2 [55040/60000 (92%)]\t training loss: 0.023512\n",
      "epoch: 2 [55360/60000 (92%)]\t training loss: 0.089568\n",
      "epoch: 2 [55680/60000 (93%)]\t training loss: 0.006522\n",
      "epoch: 2 [56000/60000 (93%)]\t training loss: 0.021059\n",
      "epoch: 2 [56320/60000 (94%)]\t training loss: 0.000819\n",
      "epoch: 2 [56640/60000 (94%)]\t training loss: 0.053035\n",
      "epoch: 2 [56960/60000 (95%)]\t training loss: 0.018449\n",
      "epoch: 2 [57280/60000 (95%)]\t training loss: 0.091526\n",
      "epoch: 2 [57600/60000 (96%)]\t training loss: 0.076476\n",
      "epoch: 2 [57920/60000 (97%)]\t training loss: 0.157255\n",
      "epoch: 2 [58240/60000 (97%)]\t training loss: 0.066362\n",
      "epoch: 2 [58560/60000 (98%)]\t training loss: 0.004818\n",
      "epoch: 2 [58880/60000 (98%)]\t training loss: 0.002790\n",
      "epoch: 2 [59200/60000 (99%)]\t training loss: 0.018160\n",
      "epoch: 2 [59520/60000 (99%)]\t training loss: 0.025157\n",
      "epoch: 2 [59840/60000 (100%)]\t training loss: 0.008034\n",
      "\n",
      "Test dataset: Overall Loss: 0.0420, Overall Accuracy: 9861/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2epoch만 출력해 본다.\n",
    "for epoch in range(1, 3):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.7 trained model로 inference하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_samples = enumerate(test_dataloader)\n",
    "b_i, (sample_data, sample_targets) = next(test_samples)\n",
    "\n",
    "plt.imshow(sample_data[0][0], cmap='gray', interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction is : 7\n",
      "Ground truth is : 7\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model prediction is : {model(sample_data).data.max(1)[1][0]}\")\n",
    "print(f\"Ground truth is : {sample_targets[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Oct 13 2022, 09:48:40) [Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
