{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 파이토치 살펴보기\n",
    "\n",
    "## 1.1 파이토치 모듈\n",
    "\n",
    "- **torch.nn**\n",
    "\n",
    "neural network architecture을 구축할 때, network가 갖는 기본 특징은 다음과 같다.\n",
    "\n",
    "- layer의 수\n",
    "\n",
    "- 각 layer의 neuron 수\n",
    "\n",
    "- 그중 학습 가능한 neuron 수 등\n",
    "\n",
    "다음 예시는 torch.nn 모듈을 사용하지 않고 perceptron을 초기화하는 코드이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# 단일 perceptron \n",
    "# input은 256차원, output은 4차원\n",
    "# 랜덤값으로 (256x4) 행렬을 initialize한다.\n",
    "weights = torch.randn(256, 4) / math.sqrt(256)\n",
    "\n",
    "# weight를 trainable하게 만든다.(autograd)\n",
    "# 즉, 256x4 matrix 값이 backpropagation을 통해 조정될 수 있게 만든다.\n",
    "# 추후 tensor의 backward()를 이용해 Jacobian matrix과 chain rule을 이용하여 backpropagation을 시도한다.\n",
    "weights.requires_grad_()\n",
    "\n",
    "# 4차원 output을 위한 bias. 이 bias도 trainable하게 설정한다.\n",
    "bias = torch.zeros(4, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주로 사용하는 tensor에 랜덤값을 채우는 function은 다음과 같다.\n",
    "\n",
    "- `torch.rand()`: 0과 1사이 숫자를 랜덤하게 생성\n",
    "\n",
    "- `torch.randn()`: mean = 0, standard deviation = 1인 **normal distribution**(정규분포, Gaussian distribution)을 이용해 랜덤값 생성\n",
    "\n",
    "> `torch.manual_seed()`을 이용해서 사용하는 seed 값을 설정할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn 모듈을 사용해 initialize하면 다음과 같다. linear regression model을 쉽게 구현할 수 있다.\n",
    "\n",
    "```Python\n",
    "# nn.Linear(input_dim,output_dim)\n",
    "nn.Linear(256, 4)\n",
    "```\n",
    "\n",
    "torch.nn 모듈에는 `torch.nn.functional`이라는 하위 모듈이 있다. 이 하위 모듈은 torch.nn 내부의 모든 function을 포함한다.(이외 다른 하위 모듈은 모두 class이다.)\n",
    "\n",
    "> loss function, activation function, functional한 방식으로 생성하기 위해 사용될 수 있는 pooling 등\n",
    "\n",
    "다음은 torch.nn.functional 모듈을 사용한 loss function의 예시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "loss = loss_func(model(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- X: input\n",
    "\n",
    "- y: target output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.optim**\n",
    "\n",
    "`torch.optim` 모듈은 **optimization**(최적화) 과정에 필요한 여러 도구와 기능을 가지고 있다. \n",
    "\n",
    "다음은 optim 모듈을 이용해 optimizer를 정의하는 예시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> SGD(Stochastic Gradient Descent, 확률적 경사하강법)\n",
    "\n",
    "> lr(learning rate, 학습률)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # stochastic gradient descent를 사용해 parameter updata를 적용한다.\n",
    "    for param in model.parameters(): param -= param.grad * lr\n",
    "    model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 'model.zero_grad()'를 적용하는 이유는 한 번의 iteration이 끝나면 gradient를 0으로 초기화하기 위함이다.(제대로 weight를 update하기 위해서 필요하다.)\n",
    "\n",
    "optim을 사용하면 다음과 같이 간단하게 작성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters를 update\n",
    "opt.step()\n",
    "\n",
    "# \n",
    "opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.utils.data**\n",
    "\n",
    "`utils.data` 모듈은 자체 data set을 제공한다. \n",
    "또한 내부의 `DatasetLoader` class를 이용하면 data 배치를 편리하게 수행할 수 있다.\n",
    "\n",
    "우선 data 배치를 수작업으로 했을 때를 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epoch가 있다면 loop를 하나 더 감싸게 된다.\n",
    "for i in range((n-1)//bs + 1):\n",
    "    start_i = i * bs\n",
    "    end_i = start_i + bs\n",
    "    \n",
    "    x_batch = x_train[start_i:end_i]\n",
    "    y_batch = y_train[start_i:end_i]\n",
    "    pred = model(x_batch)\n",
    "    loss = loss_func(pred, y_batch)\n",
    "    #..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> bs(batch size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대신 utils.data 모듈을 쓰면 다음과 같이 간단하게 code를 작성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import (TensorDataset, DataLoader)\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=bs)\n",
    "\n",
    "for x_batch, y_batch in train_dataloader:\n",
    "    pred = model(x_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.tensor**\n",
    "\n",
    "`tensor` 모듈은 numpy와 비슷하게 생각하면 된다. math function을 연산할 수 있고 GPU를 통해 speedup이 가능한 n차원 array이다.\n",
    "\n",
    "> computational graph(계산 그래프)나 gradient를 기록하는 데 사용할 수도 있다.\n",
    "\n",
    "PyTorch에서 tensor는 연속된 memory에 저장된 숫자 data의 1차원 array의 뷰로 구현된다. 이를 storage instance라고 한다.\n",
    "\n",
    "다음은 tensor를 인스턴스화하는 예시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = torch.tensor([1.0, 4.0, 2.0, 1.0, 3.0, 5.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 항목을 가져오려면 다음과 같이 하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫 번째 항목(1차원 array)을 조회한다.\n",
    "float(points[0])\n",
    "\n",
    "# tensor의 모양을 확인한다.\n",
    "points.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "storage instance를 출력하는 `storage` property를 이용해서 조회할 수도 있다. 다음 예시를 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])\n",
    "\n",
    "points.storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor를 구현하기 위해 사용된 정보(properties)들을 다음과 같이 조회할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor size 확인\n",
    "# 참고로 size를 모두 곱하면 storage instance의 총 길이를 알 수 있다.\n",
    "torch.Size([3, 2])\n",
    "\n",
    "# offset 확인\n",
    "# offset이란 storage array에서 tensor의 첫 번째 element의 index를 의미한다.\n",
    "points.storage_offset()\n",
    "\n",
    "# 다음과 같이 이용할 수 있다.\n",
    "# points[1]은 [2.0]이고, 이는 storage array에서 index 2에 위치한다.\n",
    "points[1].storage_offset()\n",
    "\n",
    "# stride 확인\n",
    "# 각 차원에서 tensor의 다음 element로 접근하기 위해 건너뛰어야 할 element 개수를 나타낸다.\n",
    "points.stride()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음과 같이 tensor에서 사용할 data type을 지정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 PyTorch의 tensor는 data를 저장할 장치를 정해야 한다.\n",
    "\n",
    "- `device='cpu'`: CPU에 할당(default)\n",
    "\n",
    "- `device='cuda'`: GPU에 할당\n",
    "\n",
    "> 현재 PyTorch는 CUDA를 지원하는 GPU만 지원한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_points = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float32, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른 장치로 tensor를 copy하는 방법은 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_points = h_points.to(device='cuda')\n",
    "\n",
    "# GPU가 여러 대 존재한다면 다음과 같이 지정할 수도 있다.\n",
    "d_points_2 = h_points.to(device='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.2 backpropagation시키기\n",
    "\n",
    "> [PyTorch로 행렬 미분하기](https://justkode.kr/deep-learning/pytorch-autograd)\n",
    "\n",
    "1. autograd 활성화시키기\n",
    "\n",
    "- 방법 1: tensor 생성 때 parameter로 `requires_grad=True` 넘겨 주기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 방법 2: `tensor`의 method인 `requires_grad_`를 이용하여 활성화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2)\n",
    "x.requires_grad_(True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. backpropagation 시키기\n",
    "\n",
    "backpropagation은 간단히 `tensor` 객체의 `backward()` method를 사용하면 된다.\n",
    "\n",
    "- 스칼라 연산\n",
    "\n",
    "다음 예제를 보며 확인해 보자. 최초에 `requires_grad=True`로 설정한 Tensor의 미분 값을 알기 위해 `grad` property를 사용해서 조회한다.\n",
    "\n",
    "$$out = z/4 = (y * y * 3)/4 = ((x+2)^{2}*3)/4$$\n",
    "\n",
    "$${{dout} \\over {dx}} = 3(2x + 4)/4$$\n",
    "\n",
    "만약 $x=1$ 일 경우\n",
    "\n",
    "$${{dout} \\over {dx}} = 4.5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "tensor(27., grad_fn=<MeanBackward0>)\n",
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x + 2\n",
    "print(y)\n",
    "z = y * y * 3\n",
    "print(z)\n",
    "out = z.mean()    # 미분 대상\n",
    "print(out)\n",
    "\n",
    "out.backward()    # out.backward(torch.tensor(1.)) 과 동일\n",
    "print(x.grad)     # dout/dx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PTtest",
   "language": "python",
   "name": "pttest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
