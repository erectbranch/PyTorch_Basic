{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 파이토치 살펴보기\n",
    "\n",
    "## 1.1 파이토치 모듈\n",
    "\n",
    "- **torch.nn**\n",
    "\n",
    "neural network architecture을 구축할 때, network가 갖는 기본 특징은 다음과 같다.\n",
    "\n",
    "- layer의 수\n",
    "\n",
    "- 각 layer의 neuron 수\n",
    "\n",
    "- 그중 학습 가능한 neuron 수 등\n",
    "\n",
    "다음 예시는 torch.nn 모듈을 사용하지 않고 perceptron을 초기화하는 코드이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# 단일 perceptron \n",
    "# input은 256차원, output은 4차원\n",
    "# 랜덤값으로 (256x4) 행렬을 initialize한다.\n",
    "weights = torch.randn(256, 4) / math.sqrt(256)\n",
    "\n",
    "# weight를 trainable하게 만든다.(autograd)\n",
    "# 즉, 256x4 matrix 값이 backpropagation을 통해 조정될 수 있게 만든다.\n",
    "# 추후 tensor의 backward()를 이용해 Jacobian matrix과 chain rule을 이용하여 backpropagation을 시도한다.\n",
    "weights.requires_grad_()\n",
    "\n",
    "# 4차원 output을 위한 bias. 이 bias도 trainable하게 설정한다.\n",
    "bias = torch.zeros(4, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주로 사용하는 tensor에 랜덤값을 채우는 function은 다음과 같다.\n",
    "\n",
    "- `torch.rand()`: 0과 1사이 숫자를 랜덤하게 생성\n",
    "\n",
    "- `torch.randn()`: mean = 0, standard deviation = 1인 **normal distribution**(정규분포, Gaussian distribution)을 이용해 랜덤값 생성\n",
    "\n",
    "> `torch.manual_seed()`을 이용해서 사용하는 seed 값을 설정할 수 있다. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn 모듈을 사용해 initialize하면 다음과 같다. linear regression model을 쉽게 구현할 수 있다.\n",
    "\n",
    "```Python\n",
    "# nn.Linear(input_dim,output_dim)\n",
    "nn.Linear(256, 4)\n",
    "```\n",
    "\n",
    "torch.nn 모듈에는 `torch.nn.functional`이라는 하위 모듈이 있다. 이 하위 모듈은 torch.nn 내부의 모든 function을 포함한다.(이외 다른 하위 모듈은 모두 class이다.)\n",
    "\n",
    "> loss function, activation function, functional한 방식으로 생성하기 위해 사용될 수 있는 pooling 등\n",
    "\n",
    "다음은 torch.nn.functional 모듈을 사용한 loss function의 예시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "loss = loss_func(model(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- X: input\n",
    "\n",
    "- y: target output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.optim**\n",
    "\n",
    "`torch.optim` 모듈은 **optimization**(최적화) 과정에 필요한 여러 도구와 기능을 가지고 있다. \n",
    "\n",
    "다음은 optim 모듈을 이용해 optimizer를 정의하는 예시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> SGD(Stochastic Gradient Descent, 확률적 경사하강법)\n",
    "\n",
    "> lr(learning rate, 학습률)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # stochastic gradient descent를 사용해 parameter updata를 적용한다.\n",
    "    for param in model.parameters(): param -= param.grad * lr\n",
    "    model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 'model.zero_grad()'를 적용하는 이유는 한 번의 iteration이 끝나면 gradient를 0으로 초기화하기 위함이다.(제대로 weight를 update하기 위해서 필요하다.)\n",
    "\n",
    "optim을 사용하면 다음과 같이 간단하게 작성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters를 update\n",
    "opt.step()\n",
    "\n",
    "# \n",
    "opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.utils.data**\n",
    "\n",
    "`utils.data` 모듈은 자체 data set을 제공한다. \n",
    "또한 내부의 `DatasetLoader` class를 이용하면 data 배치를 편리하게 수행할 수 있다.\n",
    "\n",
    "우선 data 배치를 수작업으로 했을 때를 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epoch가 있다면 loop를 하나 더 감싸게 된다.\n",
    "for i in range((n-1)//bs + 1):\n",
    "    start_i = i * bs\n",
    "    end_i = start_i + bs\n",
    "    \n",
    "    x_batch = x_train[start_i:end_i]\n",
    "    y_batch = y_train[start_i:end_i]\n",
    "    pred = model(x_batch)\n",
    "    loss = loss_func(pred, y_batch)\n",
    "    #..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> bs(batch size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대신 utils.data 모듈을 쓰면 다음과 같이 간단하게 code를 작성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import (TensorDataset, DataLoader)\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=bs)\n",
    "\n",
    "for x_batch, y_batch in train_dataloader:\n",
    "    pred = model(x_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **torch.tensor**\n",
    "\n",
    "`tensor` 모듈은 numpy와 비슷하게 생각하면 된다. math function을 연산할 수 있고 GPU를 통해 speedup이 가능한 n차원 array이다.\n",
    "\n",
    "> computational graph(계산 그래프)나 gradient를 기록하는 데 사용할 수도 있다.\n",
    "\n",
    "PyTorch에서 tensor는 연속된 memory에 저장된 숫자 data의 1차원 array의 뷰로 구현된다. 이를 storage instance라고 한다.\n",
    "\n",
    "다음은 tensor를 인스턴스화하는 예시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = torch.tensor([1.0, 4.0, 2.0, 1.0, 3.0, 5.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 항목을 가져오려면 다음과 같이 하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫 번째 항목(1차원 array)을 조회한다.\n",
    "float(points[0])\n",
    "\n",
    "# tensor의 모양을 확인한다.\n",
    "points.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "storage instance를 출력하는 `storage` property를 이용해서 조회할 수도 있다. 다음 예시를 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])\n",
    "\n",
    "points.storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor를 구현하기 위해 사용된 정보(properties)들을 다음과 같이 조회할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor size 확인\n",
    "# 참고로 size를 모두 곱하면 storage instance의 총 길이를 알 수 있다.\n",
    "torch.Size([3, 2])\n",
    "\n",
    "# offset 확인\n",
    "# offset이란 storage array에서 tensor의 첫 번째 element의 index를 의미한다.\n",
    "points.storage_offset()\n",
    "\n",
    "# 다음과 같이 이용할 수 있다.\n",
    "# points[1]은 [2.0]이고, 이는 storage array에서 index 2에 위치한다.\n",
    "points[1].storage_offset()\n",
    "\n",
    "# stride 확인\n",
    "# 각 차원에서 tensor의 다음 element로 접근하기 위해 건너뛰어야 할 element 개수를 나타낸다.\n",
    "points.stride()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음과 같이 tensor에서 사용할 data type을 지정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 PyTorch의 tensor는 data를 저장할 장치를 정해야 한다.\n",
    "\n",
    "- `device='cpu'`: CPU에 할당(default)\n",
    "\n",
    "- `device='cuda'`: GPU에 할당\n",
    "\n",
    "> 현재 PyTorch는 CUDA를 지원하는 GPU만 지원한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_points = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float32, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른 장치로 tensor를 copy하는 방법은 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_points = h_points.to(device='cuda')\n",
    "\n",
    "# GPU가 여러 대 존재한다면 다음과 같이 지정할 수도 있다.\n",
    "d_points_2 = h_points.to(device='cuda:0')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.2 backpropagation시키기\n",
    "\n",
    "1. autograd 활성화시키기\n",
    "\n",
    "- 방법 1: tensor 생성 때 parameter로 `requires_grad=True` 넘겨 주기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m, requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(x)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
